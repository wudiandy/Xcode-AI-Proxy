"""
Xcode AI Proxy - Python ç‰ˆæœ¬
ä½¿ç”¨ FastAPI é‡å†™çš„ AI ä»£ç†æœåŠ¡ï¼Œæ”¯æŒæ™ºè°± GLM-4.6ã€Kimi å’Œ DeepSeek æ¨¡å‹
æ ¹æ®ç¯å¢ƒå˜é‡åŠ¨æ€åŠ è½½å¯ç”¨æ¨¡å‹
"""

import os
import sys
import asyncio
import logging
import time
import uuid
from datetime import datetime
from typing import Dict, Any, Optional, Union
import json

import httpx
from fastapi import FastAPI, Request, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import StreamingResponse, JSONResponse
from pydantic import BaseModel
from dotenv import load_dotenv
import uvicorn

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s",
    handlers=[logging.StreamHandler(sys.stdout)],
)
logger = logging.getLogger(__name__)

# æœåŠ¡å™¨é…ç½®
PORT = int(os.getenv("PORT", 3000))
HOST = os.getenv("HOST", "0.0.0.0")

# é‡è¯•é…ç½®
MAX_RETRIES = int(os.getenv("MAX_RETRIES", 3))
RETRY_DELAY = int(os.getenv("RETRY_DELAY", 1000)) / 1000  # è½¬æ¢ä¸ºç§’
REQUEST_TIMEOUT = int(os.getenv("REQUEST_TIMEOUT", 60000)) / 1000  # è½¬æ¢ä¸ºç§’

# æ£€æŸ¥å¿…éœ€çš„ç¯å¢ƒå˜é‡
REQUIRED_ENV_VARS = {
    "ZHIPU_API_KEY": "GLM-4.6 æ¨¡å‹",
    "KIMI_API_KEY": "Kimi æ¨¡å‹",
    "DEEPSEEK_API_KEY": "DeepSeek æ¨¡å‹",
}

# æ£€æŸ¥æ‰€æœ‰ç¯å¢ƒå˜é‡ï¼Œä½†åªç»™å‡ºè­¦å‘Šè€Œä¸é€€å‡º
for env_var, model_name in REQUIRED_ENV_VARS.items():
    if not os.getenv(env_var):
        logger.warning(f"âš ï¸ ç¼ºå°‘ç¯å¢ƒå˜é‡ {env_var} (ç”¨äº {model_name})ï¼Œè¯¥æ¨¡å‹å°†ä¸å¯ç”¨")

# API é…ç½® - æ ¹æ®ç¯å¢ƒå˜é‡åŠ¨æ€æ·»åŠ æ¨¡å‹
API_CONFIGS = {}

# å¦‚æœæœ‰æ™ºè°± API å¯†é’¥ï¼Œåˆ™æ·»åŠ æ™ºè°±æ¨¡å‹é…ç½®
if os.getenv("ZHIPU_API_KEY"):
    API_CONFIGS["glm-4.6"] = {
        "api_url": "https://open.bigmodel.cn/api/paas/v4",
        "api_key": os.getenv("ZHIPU_API_KEY"),
        "type": "zhipu",
        "name": "GLM-4.6",
    }

# å¦‚æœæœ‰ Kimi API å¯†é’¥ï¼Œåˆ™æ·»åŠ  Kimi æ¨¡å‹é…ç½®
if os.getenv("KIMI_API_KEY"):
    API_CONFIGS["kimi-k2-0905-preview"] = {
        "api_url": "https://api.moonshot.cn/v1",
        "api_key": os.getenv("KIMI_API_KEY"),
        "type": "kimi",
        "name": "Kimi K2",
    }

# å¦‚æœæœ‰ DeepSeek API å¯†é’¥ï¼Œåˆ™æ·»åŠ  DeepSeek æ¨¡å‹é…ç½®
if os.getenv("DEEPSEEK_API_KEY"):
    API_CONFIGS.update(
        {
            "deepseek-reasoner": {
                "api_url": "https://api.deepseek.com",
                "api_key": os.getenv("DEEPSEEK_API_KEY"),
                "type": "deepseek",
                "name": "DeepSeek Reasoner",
            },
            "deepseek-chat": {
                "api_url": "https://api.deepseek.com",
                "api_key": os.getenv("DEEPSEEK_API_KEY"),
                "type": "deepseek",
                "name": "DeepSeek Chat",
            },
        }
    )

if not API_CONFIGS:
    logger.error("âŒ æœªé…ç½®ä»»ä½•æ¨¡å‹APIå¯†é’¥ï¼Œè¯·è‡³å°‘è®¾ç½®ä¸€ä¸ªç¯å¢ƒå˜é‡:")
    for env_var, model_name in REQUIRED_ENV_VARS.items():
        logger.error(f"   - {env_var} (ç”¨äº {model_name})")
    logger.error("è¯·è®¾ç½®ç›¸åº”çš„ç¯å¢ƒå˜é‡åé‡æ–°å¯åŠ¨æœåŠ¡")
    sys.exit(1)

logger.info("ğŸ“‹ å·²åŠ è½½æ¨¡å‹é…ç½®:")
for model_id, config in API_CONFIGS.items():
    logger.info(f"   âœ… {model_id} ({config['name']}) - å·²é…ç½®")

# FastAPI åº”ç”¨åˆå§‹åŒ–
app = FastAPI(
    title="Xcode AI Proxy",
    description="AI ä»£ç†æœåŠ¡ï¼Œæ”¯æŒæ™ºè°± GLM-4.6ã€Kimi å’Œ DeepSeek æ¨¡å‹",
    version="1.0.0",
)

# æ·»åŠ  CORS ä¸­é—´ä»¶
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)


# è¯·æ±‚æ¨¡å‹
class ChatCompletionRequest(BaseModel):
    model: str
    messages: list
    stream: bool = False
    temperature: Optional[float] = None
    max_tokens: Optional[int] = None
    top_p: Optional[float] = None


# é€šç”¨é‡è¯•è£…é¥°å™¨
async def with_retry(operation, max_retries=MAX_RETRIES, base_delay=RETRY_DELAY):
    """é€šç”¨å¼‚æ­¥é‡è¯•å‡½æ•°"""
    last_error = None

    for attempt in range(1, max_retries + 1):
        try:
            logger.info(f"ğŸ”„ ç¬¬{attempt}æ¬¡å°è¯•")
            return await operation()
        except Exception as error:
            last_error = error
            logger.error(f"âŒ ç¬¬{attempt}æ¬¡å°è¯•å¤±è´¥: {str(error)}")

            if attempt < max_retries:
                delay = base_delay * attempt  # é€’å¢å»¶è¿Ÿ
                logger.info(f"â³ {delay}ç§’åé‡è¯•...")
                await asyncio.sleep(delay)

    logger.error(f"âŒ æ‰€æœ‰{max_retries}æ¬¡é‡è¯•éƒ½å¤±è´¥äº†")
    # å¦‚æœæ²¡æœ‰æ•è·åˆ°å…·ä½“å¼‚å¸¸ï¼Œé¿å… raise Noneï¼Œæä¾›ä¸€ä¸ªæ˜ç¡®çš„å›é€€é”™è¯¯
    if last_error:
        raise last_error
    else:
        raise RuntimeError("Operation failed after retries with no exception captured")


# ä¸­é—´ä»¶ï¼šè¯·æ±‚æ—¥å¿—
@app.middleware("http")
async def log_requests(request: Request, call_next):
    start_time = datetime.now()
    logger.info(f"{start_time.isoformat()} - {request.method} {request.url.path}")

    # è®°å½•è¯·æ±‚å¤´
    logger.info(f"è¯·æ±‚å¤´: {dict(request.headers)}")

    response = await call_next(request)

    process_time = (datetime.now() - start_time).total_seconds()
    logger.info(f"è¯·æ±‚å¤„ç†æ—¶é—´: {process_time:.3f}ç§’")
    logger.info(f"å“åº”çŠ¶æ€ç : {response.status_code}")

    return response


# å¥åº·æ£€æŸ¥
@app.get("/health")
async def health_check():
    """å¥åº·æ£€æŸ¥æ¥å£"""
    return {"status": "ok", "timestamp": datetime.now().isoformat()}


# è°ƒè¯•ç«¯ç‚¹
@app.get("/debug/config")
async def debug_config():
    """è°ƒè¯•é…ç½®ä¿¡æ¯"""
    return {
        "available_models": list(API_CONFIGS.keys()),
        "config_summary": {
            model_id: {
                "name": config["name"],
                "type": config["type"],
                "api_url": config["api_url"],
                "has_api_key": bool(config.get("api_key")),
            }
            for model_id, config in API_CONFIGS.items()
        },
    }


# æ¨¡å‹åˆ—è¡¨
@app.get("/v1/models")
async def list_models():
    """è¿”å›æ”¯æŒçš„æ¨¡å‹åˆ—è¡¨"""
    logger.info("ğŸ“‹ è¿”å›æ¨¡å‹åˆ—è¡¨")

    model_list = [
        {
            "id": model_id,
            "object": "model",
            "created": 1677610602,
            "owned_by": config["type"],
            "name": config.get("name", model_id),
        }
        for model_id, config in API_CONFIGS.items()
    ]

    return {"object": "list", "data": model_list}


# æ™ºè°± API å¤„ç†
async def handle_zhipu_request(request_body: dict) -> Union[dict, StreamingResponse]:
    """å¤„ç†æ™ºè°± API è¯·æ±‚"""
    logger.info("ğŸ“¡ è·¯ç”±åˆ°æ™ºè°±API")

    async def make_request():
        config = API_CONFIGS["glm-4.6"]

        async with httpx.AsyncClient(timeout=REQUEST_TIMEOUT) as client:
            response = await client.post(
                f"{config['api_url']}/chat/completions",
                json={**request_body, "model": "glm-4.6"},
                headers={
                    "Authorization": f"Bearer {config['api_key']}",
                    "Content-Type": "application/json",
                },
            )
            # é 2xx çŠ¶æ€ä¼šè§¦å‘ raise_for_status() æŠ›å‡º HTTPStatusError
            response.raise_for_status()
            return response

    response = await with_retry(make_request)
    logger.info(f"âœ… æ™ºè°±APIå“åº”çŠ¶æ€: {response.status_code}")

    if request_body.get("stream", False):
        logger.info("ğŸ”„ è¿”å›æ™ºè°±æµå¼å“åº”")

        # ç›´æ¥è¿”å›åŸå§‹æµå¼å“åº”ï¼Œä¸ä¿®æ”¹ä»»ä½•å†…å®¹
        response_headers = dict(response.headers)
        # ç§»é™¤å¯èƒ½å¼•èµ·é—®é¢˜çš„å¤´éƒ¨
        response_headers.pop("content-length", None)
        response_headers.pop("content-encoding", None)

        async def generate():
            async for chunk in response.aiter_bytes(chunk_size=8192):
                yield chunk

        return StreamingResponse(
            generate(), status_code=response.status_code, headers=response_headers
        )
    else:
        logger.info("ğŸ“¦ è¿”å›æ™ºè°±éæµå¼å“åº”")
        return response.json()


# Kimi API å¤„ç†
async def handle_kimi_request(request_body: dict) -> Union[dict, StreamingResponse]:
    """å¤„ç† Kimi API è¯·æ±‚"""
    logger.info("ğŸ“¡ è·¯ç”±åˆ°Kimi API")

    async def make_request():
        config = API_CONFIGS["kimi-k2-0905-preview"]

        async with httpx.AsyncClient(timeout=REQUEST_TIMEOUT) as client:
            response = await client.post(
                f"{config['api_url']}/chat/completions",
                json={**request_body, "model": "kimi-k2-0905-preview"},
                headers={
                    "Authorization": f"Bearer {config['api_key']}",
                    "Content-Type": "application/json",
                },
            )
            # é 2xx çŠ¶æ€ä¼šè§¦å‘ raise_for_status() æŠ›å‡º HTTPStatusError
            response.raise_for_status()
            return response

    response = await with_retry(make_request)
    logger.info(f"âœ… Kimi APIå“åº”çŠ¶æ€: {response.status_code}")

    if request_body.get("stream", False):
        logger.info("ğŸ”„ è¿”å›Kimiæµå¼å“åº”")

        # ç›´æ¥è¿”å›åŸå§‹æµå¼å“åº”ï¼Œä¸ä¿®æ”¹ä»»ä½•å†…å®¹
        response_headers = dict(response.headers)
        # ç§»é™¤å¯èƒ½å¼•èµ·é—®é¢˜çš„å¤´éƒ¨
        response_headers.pop("content-length", None)
        response_headers.pop("content-encoding", None)

        async def generate():
            async for chunk in response.aiter_bytes(chunk_size=8192):
                yield chunk

        return StreamingResponse(
            generate(), status_code=response.status_code, headers=response_headers
        )
    else:
        logger.info("ğŸ“¦ è¿”å›Kimiéæµå¼å“åº”")
        return response.json()


# æ–°å¢ï¼šæ¸…æ´— messagesï¼Œç¡®ä¿æ¯æ¡ message['content'] ä¸ºå­—ç¬¦ä¸²
def sanitize_messages(messages):
    """
    ç¡®ä¿ messages æ˜¯ listï¼Œæ¯ä¸ª message ä¸º dict ä¸” message['content'] ä¸ºå­—ç¬¦ä¸²ã€‚
    - å¦‚æœ message æ˜¯å­—ç¬¦ä¸² -> è½¬ä¸º {'role':'user','content': str}
    - å¦‚æœ content æ˜¯ list -> å°†å…ƒç´  joinï¼ˆéå­—ç¬¦ä¸²å…ƒç´  json.dumpsï¼‰
    - å…¶ä»–éå­—ç¬¦ä¸² -> json.dumps
    """
    import json

    if not isinstance(messages, list):
        logger.warning("messages ä¸æ˜¯åˆ—è¡¨ï¼Œå·²å°è¯•è½¬æ¢ä¸ºå•é¡¹åˆ—è¡¨")
        return [{"role": "user", "content": str(messages)}]

    sanitized = []
    for idx, m in enumerate(messages):
        # å­—ç¬¦ä¸²å½¢å¼çš„ messageï¼Œè§†ä¸º user
        if isinstance(m, str):
            sanitized.append({"role": "user", "content": m})
            continue

        if not isinstance(m, dict):
            # æ— æ³•è¯†åˆ«çš„ç±»å‹ï¼Œåºåˆ—åŒ–ä¸ºå­—ç¬¦ä¸²
            sanitized.append(
                {"role": "user", "content": json.dumps(m, ensure_ascii=False)}
            )
            continue

        content = m.get("content", "")
        if isinstance(content, str):
            s = content
        elif isinstance(content, list):
            parts = []
            for part in content:
                if isinstance(part, str):
                    parts.append(part)
                else:
                    parts.append(json.dumps(part, ensure_ascii=False))
            s = "\n".join(parts)
        else:
            s = json.dumps(content, ensure_ascii=False)

        new_m = {**m, "content": s}
        sanitized.append(new_m)

    return sanitized


async def parse_sse_stream(resp: httpx.Response) -> str:
    """è§£æ response çš„ SSE æµï¼Œå¹¶ä¸”æŠŠè§£æçš„ç»“æœæš‚æ—¶å­˜åˆ°æœ¬åœ°å­—ç¬¦ä¸²ä¸­"""
    buffer = ""
    fragments = []

    async for chunk in resp.aiter_text(chunk_size=8192):
        buffer += chunk

        while "\n\n" in buffer:
            event, buffer = buffer.split("\n\n", 1)
            if not event.strip():
                continue

            for line in event.splitlines():
                if not line.startswith("data:"):
                    continue

                data = line[5:].strip()
                if not data:
                    continue
                if data == "[DONE]":
                    return "".join(fragments)

                try:
                    payload = json.loads(data)
                except json.JSONDecodeError:
                    fragments.append(data)
                    continue

                if isinstance(payload, dict):
                    choices = payload.get("choices") or []
                    for choice in choices:
                        delta = choice.get("delta") or {}
                        message = choice.get("message") or {}
                        for block in (delta, message):
                            content_piece = block.get("content")
                            if content_piece:
                                fragments.append(content_piece)

                    if not choices and payload.get("content"):
                        content_value = payload["content"]
                        if isinstance(content_value, str):
                            fragments.append(content_value)
                        else:
                            fragments.append(json.dumps(content_value, ensure_ascii=False))
                else:
                    fragments.append(str(payload))

    return "".join(fragments)


def process_parsed_stream_cache(parsed_stream_cache: str) -> str:
    """å¯¹ parsed_stream_cache è¿›è¡Œå¤„ç†"""
    try:
        payload = json.loads(parsed_stream_cache)
    except json.JSONDecodeError:
        return parsed_stream_cache

    try:
        json.loads(payload.get("text", ""))
        return process_parsed_stream_cache(payload.get("text", ""))
    except (json.JSONDecodeError, AttributeError):
        return payload.get("text", "")


# DeepSeek API å¤„ç†
async def handle_deepseek_request(request_body: dict) -> Union[dict, StreamingResponse]:
    """å¤„ç† DeepSeek API è¯·æ±‚"""
    logger.info("ğŸ“¡ è·¯ç”±åˆ°DeepSeek API")
    
    request_body['messages'] = sanitize_messages(request_body['messages'])
    logger.info('ğŸ§¹ åœ¨ handle_proxy ä¸­å·²æ¸…æ´— messages')
    
    model = request_body.get("model", "deepseek-reasoner")
    logger.info(f"ğŸ” ä½¿ç”¨ DeepSeek æ¨¡å‹: {model}")

    async def make_request():
        config = API_CONFIGS[model]

        # è¿‡æ»¤ DeepSeek API æ”¯æŒçš„å‚æ•°
        supported_params = {
            "model",
            "messages",
            "stream",
            "temperature",
            "max_tokens",
            "top_p",
            "frequency_penalty",
            "presence_penalty",
            "stop",
        }

        # æ„å»ºæ¸…ç†åçš„è¯·æ±‚æ•°æ®
        request_data = {
            key: value for key, value in request_body.items() if key in supported_params
        }

        # ç¡®ä¿æ¨¡å‹åç§°æ­£ç¡®
        request_data["model"] = model

        # ç§»é™¤ç©ºçš„æ•°ç»„å‚æ•°
        if "tools" in request_body and not request_body["tools"]:
            logger.info("ğŸ§¹ ç§»é™¤ç©ºçš„ tools å‚æ•°")

        # è®°å½•è¿‡æ»¤çš„å‚æ•°
        filtered_params = set(request_body.keys()) - set(request_data.keys())
        if filtered_params:
            logger.info(f"ğŸ§¹ å·²è¿‡æ»¤ä¸æ”¯æŒçš„å‚æ•°: {filtered_params}")

        logger.info(f'ğŸ“¤ å‘é€åˆ° DeepSeek API: {config["api_url"]}/chat/completions')
        logger.info(f"ğŸ“‹ è¯·æ±‚å‚æ•°: {list(request_data.keys())}")

        async with httpx.AsyncClient(timeout=REQUEST_TIMEOUT) as client:
            response = await client.post(
                f"{config['api_url']}/chat/completions",
                json=request_data,
                headers={
                    "Authorization": f"Bearer {config['api_key']}",
                    "Content-Type": "application/json",
                },
            )

            # è®°å½•å“åº”çŠ¶æ€å’Œé”™è¯¯ä¿¡æ¯
            logger.info(f"ğŸ“¥ DeepSeek API å“åº”çŠ¶æ€: {response.status_code}")
            if response.status_code != 200:
                response_text = response.text
                logger.error(f"âŒ DeepSeek API é”™è¯¯å“åº”: {response_text}")

            # é 2xx çŠ¶æ€ä¼šè§¦å‘ raise_for_status() æŠ›å‡º HTTPStatusError
            response.raise_for_status()
            return response

    response = await with_retry(make_request)
    logger.info(f"âœ… DeepSeek APIå“åº”çŠ¶æ€: {response.status_code}")

    if request_body.get("stream", False):
        logger.info("ğŸ”„ è¿”å›DeepSeekæµå¼å“åº”")

        # ç›´æ¥è¿”å›åŸå§‹æµå¼å“åº”ï¼Œä¸ä¿®æ”¹ä»»ä½•å†…å®¹
        response_headers = dict(response.headers)
        # ç§»é™¤å¯èƒ½å¼•èµ·é—®é¢˜çš„å¤´éƒ¨
        response_headers.pop("content-length", None)
        response_headers.pop("content-encoding", None)
        response_headers["content-type"] = "text/event-stream; charset=utf-8"

        # è§£æ response çš„ SSE æµï¼Œå¹¶ä¸”æŠŠè§£æçš„ç»“æœæš‚æ—¶å­˜åˆ°æœ¬åœ°å­—ç¬¦ä¸²ä¸­
        parsed_stream_cache = await parse_sse_stream(response)
        logger.info(f"ğŸ§© DeepSeekæµå¼ç¼“å­˜è§£æç»“æœ: {parsed_stream_cache!r}")

        # å¯¹ parsed_stream_cache è¿›è¡Œå¤„ç†ã€‚
        parsed_stream_cache = process_parsed_stream_cache(parsed_stream_cache)
        logger.info(f"ğŸ§© DeepSeekæµå¼ç¼“å­˜å¤„ç†åç»“æœ: {parsed_stream_cache!r}")

        async def generate():
            # å°†è§£æåçš„æ–‡æœ¬æ‹†åˆ†ä¸ºå¤šä¸ª SSE å—å¹¶é€ä¸ªæ¨é€
            chunk_size = 1024
            text = parsed_stream_cache or ""
            stream_id = str(uuid.uuid4())
            system_fingerprint = "fp_proxy_stream"
            for index, start in enumerate(range(0, len(text), chunk_size)):
                segment = text[start : start + chunk_size]
                payload = {
                    "id": stream_id,
                    "object": "chat.completion.chunk",
                    "created": int(time.time()),
                    "model": model,
                    "system_fingerprint": system_fingerprint,
                    "choices": [
                        {
                            "index": 0,
                            "delta": {"content": segment},
                            "logprobs": None,
                            "finish_reason": None,
                        }
                    ],
                }
                sse_chunk = f"data: {json.dumps(payload, ensure_ascii=False)}\n\n"
                logger.debug(f"ğŸ”€ å‘é€SSEå—(index={index}): {sse_chunk!r}")
                yield sse_chunk.encode("utf-8")
                await asyncio.sleep(0)

            # å‘é€ç»“æŸå—ï¼ŒæŒ‡ç¤ºå®Œæˆ
            finish_payload = {
                "id": stream_id,
                "object": "chat.completion.chunk",
                "created": int(time.time()),
                "model": model,
                "system_fingerprint": system_fingerprint,
                "choices": [
                    {
                        "index": 0,
                        "delta": {},
                        "logprobs": None,
                        "finish_reason": "stop",
                    }
                ],
            }
            finish_chunk = f"data: {json.dumps(finish_payload, ensure_ascii=False)}\n\n"
            logger.debug(f"ğŸ å‘é€SSEç»“æŸå—: {finish_chunk!r}")
            yield finish_chunk.encode("utf-8")
            yield b"data: [DONE]\n\n"

        return StreamingResponse(
            generate(), status_code=response.status_code, headers=response_headers
        )
    else:
        logger.info("ğŸ“¦ è¿”å›DeepSeekéæµå¼å“åº”")
        return response.json()  # ä»£ç†å¤„ç†å‡½æ•°


async def handle_proxy(request_data: dict):
    """å¤„ç†ä»£ç†è¯·æ±‚"""
    try:
        model = request_data.get("model")
        logger.info(f"ğŸ¯ è¯·æ±‚æ¨¡å‹: {model}")
        logger.info(f'ğŸ” æ˜¯å¦æµå¼: {request_data.get("stream", False)}')

        if not model or model not in API_CONFIGS:
            raise HTTPException(
                status_code=400,
                detail={
                    "error": {
                        "message": f"ä¸æ”¯æŒçš„æ¨¡å‹: {model}ã€‚æ”¯æŒçš„æ¨¡å‹: {', '.join(API_CONFIGS.keys())}",
                        "type": "invalid_request_error",
                    }
                },
            )

        config = API_CONFIGS[model]

        if config["type"] == "zhipu":
            return await handle_zhipu_request(request_data)
        elif config["type"] == "kimi":
            return await handle_kimi_request(request_data)
        elif config["type"] == "deepseek":
            return await handle_deepseek_request(request_data)
        else:
            raise HTTPException(
                status_code=500,
                detail={
                    "error": {
                        "message": f"æœªçŸ¥çš„æ¨¡å‹ç±»å‹: {config['type']}",
                        "type": "internal_error",
                    }
                },
            )

    except HTTPException:
        raise
    except httpx.HTTPStatusError as error:
        logger.error(
            f"âŒ HTTP çŠ¶æ€é”™è¯¯: {error.response.status_code} - {error.response.text}"
        )
        raise HTTPException(
            status_code=error.response.status_code,
            detail={
                "error": {
                    "message": f"API è¯·æ±‚å¤±è´¥: {error.response.status_code} - {error.response.text}",
                    "type": "api_error",
                }
            },
        )
    except httpx.RequestError as error:
        logger.error(f"âŒ è¯·æ±‚é”™è¯¯: {str(error)}")
        raise HTTPException(
            status_code=500,
            detail={
                "error": {
                    "message": f"ç½‘ç»œè¯·æ±‚å¤±è´¥: {str(error)}",
                    "type": "network_error",
                }
            },
        )
    except Exception as error:
        logger.error(f"âŒ ä»£ç†è¯·æ±‚å¤±è´¥: {str(error)}")
        raise HTTPException(
            status_code=500,
            detail={"error": {"message": str(error), "type": "proxy_error"}},
        )


# Chat Completions æ¥å£
@app.post("/v1/chat/completions")
async def chat_completions(request: Request):
    """OpenAI å…¼å®¹çš„èŠå¤©å®Œæˆæ¥å£"""
    try:
        body = await request.json()
        logger.info(f"è¯·æ±‚ä½“: {body}")

        # éªŒè¯å¿…éœ€å­—æ®µ
        if "model" not in body:
            logger.error("è¯·æ±‚ä½“ç¼ºå°‘ 'model' å­—æ®µ")
            raise HTTPException(
                status_code=400,
                detail={
                    "error": {
                        "message": "Missing required field: 'model'",
                        "type": "invalid_request_error",
                    }
                },
            )

        if "messages" not in body:
            logger.error("è¯·æ±‚ä½“ç¼ºå°‘ 'messages' å­—æ®µ")
            raise HTTPException(
                status_code=400,
                detail={
                    "error": {
                        "message": "Missing required field: 'messages'",
                        "type": "invalid_request_error",
                    }
                },
            )

        return await handle_proxy(body)
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"è§£æè¯·æ±‚ä½“å¤±è´¥: {str(e)}")
        raise HTTPException(
            status_code=400,
            detail={
                "error": {
                    "message": f"Invalid request body: {str(e)}",
                    "type": "invalid_request_error",
                }
            },
        )


@app.post("/api/v1/chat/completions")
async def api_chat_completions(request: Request):
    """å¤‡ç”¨èŠå¤©å®Œæˆæ¥å£"""
    try:
        body = await request.json()
        logger.info(f"APIæ¥å£è¯·æ±‚ä½“: {body}")
        return await handle_proxy(body)
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"APIæ¥å£è§£æè¯·æ±‚ä½“å¤±è´¥: {str(e)}")
        raise HTTPException(
            status_code=400,
            detail={
                "error": {
                    "message": f"Invalid request body: {str(e)}",
                    "type": "invalid_request_error",
                }
            },
        )


@app.post("/v1/messages")
async def messages(request: Request):
    """æ¶ˆæ¯æ¥å£"""
    try:
        body = await request.json()
        logger.info(f"æ¶ˆæ¯æ¥å£è¯·æ±‚ä½“: {body}")
        return await handle_proxy(body)
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"æ¶ˆæ¯æ¥å£è§£æè¯·æ±‚ä½“å¤±è´¥: {str(e)}")
        raise HTTPException(
            status_code=400,
            detail={
                "error": {
                    "message": f"Invalid request body: {str(e)}",
                    "type": "invalid_request_error",
                }
            },
        )


# å¯åŠ¨å‡½æ•°
def main():
    """å¯åŠ¨æœåŠ¡å™¨"""
    logger.info("ğŸš€ Xcode AI ä»£ç†æœåŠ¡å·²å¯åŠ¨")
    logger.info(f"ğŸ“¡ ç›‘å¬åœ°å€: http://{HOST}:{PORT}")
    logger.info("ğŸ¯ å½“å‰å¯ç”¨çš„æ¨¡å‹:")
    for model, config in API_CONFIGS.items():
        logger.info(f"   âœ… {model} ({config.get('name', config['type'])})")

    if not API_CONFIGS:
        logger.error("âŒ æ²¡æœ‰å¯ç”¨çš„æ¨¡å‹ï¼Œè¯·æ£€æŸ¥ç¯å¢ƒå˜é‡é…ç½®")
        return

    logger.info("âš™ï¸ é‡è¯•é…ç½®:")
    logger.info(f"   æœ€å¤§é‡è¯•æ¬¡æ•°: {MAX_RETRIES}")
    logger.info(f"   é‡è¯•å»¶è¿Ÿ: {int(RETRY_DELAY * 1000)}ms (é€’å¢)")
    logger.info(f"   è¯·æ±‚è¶…æ—¶: {int(REQUEST_TIMEOUT * 1000)}ms")

    logger.info("ğŸ“‹ é…ç½® Xcode:")
    logger.info(f"   ANTHROPIC_BASE_URL: http://localhost:{PORT}")
    logger.info("   ANTHROPIC_AUTH_TOKEN: any-string-works")
    logger.info("ğŸ”§ åŠŸèƒ½: æ™ºè°±/Kimi/DeepSeekä»£ç†ï¼Œæµå¼å“åº”ï¼ŒåŠ¨æ€é…ç½®ï¼Œæ™ºèƒ½é‡è¯•")

    uvicorn.run(
        "server:app", host=HOST, port=PORT, reload=False, log_level="info"
    )


if __name__ == "__main__":
    main()
